{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\n",
    "from keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder='data/3-processed_data'\n",
    "out_folder='models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(in_folder+'/X_train.csv')\n",
    "Y_train=pd.read_csv(in_folder+'/Y_train.csv')\n",
    "X_test=pd.read_csv(in_folder+'/X_test.csv')\n",
    "Y_test=pd.read_csv(in_folder+'/Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words we want to use (or: number of rows in incoming embedding vector)\n",
    "max_features = 20000 \n",
    "\n",
    "# max number of words in a comment to use (or: number of columns in incoming embedding vector)\n",
    "max_len = 200 \n",
    "\n",
    "# dimension of the embedding variable (or: number of rows in output of embedding vector)\n",
    "embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate CNN model\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# add embedding layer \n",
    "cnn_model.add(Embedding(input_dim=max_features, input_length=max_len,\n",
    "                        output_dim=embedding_dims))\n",
    " \n",
    "# set the dropout layer to drop out 50% of the nodes\n",
    "cnn_model.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# add convolutional layer that has ...\n",
    "# ... 100 filters with a kernel size of 4 so that each convolution will consider a window of 4 word embeddings\n",
    "cnn_model.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))\n",
    "\n",
    "# add normalization layer\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "# add pooling layer \n",
    "cnn_model.add(GlobalMaxPool1D())\n",
    "\n",
    "# set the dropout layer to drop out 50% of the nodes\n",
    "cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# add dense layer to produce an output dimension of 50 and using relu activation\n",
    "cnn_model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# finally add a dense layer\n",
    "cnn_model.add(Dense(7, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 200, 100)          51300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,617,107\n",
      "Trainable params: 2,616,907\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(0.01),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/4\n",
      "127656/127656 [==============================] - 297s 2ms/step - loss: 0.0984 - acc: 0.9686 - val_loss: 0.0663 - val_acc: 0.9769\n",
      "Epoch 2/4\n",
      "127656/127656 [==============================] - 323s 3ms/step - loss: 0.0637 - acc: 0.9773 - val_loss: 0.0629 - val_acc: 0.9777\n",
      "Epoch 3/4\n",
      "127656/127656 [==============================] - 329s 3ms/step - loss: 0.0569 - acc: 0.9792 - val_loss: 0.0639 - val_acc: 0.9778\n",
      "Epoch 4/4\n",
      "127656/127656 [==============================] - 315s 2ms/step - loss: 0.0529 - acc: 0.9803 - val_loss: 0.0657 - val_acc: 0.9775\n"
     ]
    }
   ],
   "source": [
    "cnn_hist = cnn_model.fit(X_train, Y_train, batch_size=256, \n",
    "                         epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 51s 796us/step\n",
      "Test Loss:     0.08784714866866392\n",
      "Test Accuracy: 0.9653452397652403\n"
     ]
    }
   ],
   "source": [
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print('Test Loss:    ', cnn_test_loss)\n",
    "print('Test Accuracy:', cnn_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "cnn_model.save(out_folder+'/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Performance\n",
    "models_performance=pd.read_csv('models/models_performance.csv')\n",
    "stats=['Convolutional Model', cnn_test_loss,cnn_test_acc]\n",
    "models_performance.loc[len(models_performance),:]=stats\n",
    "models_performance.to_csv(out_folder+'/models_performance.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
