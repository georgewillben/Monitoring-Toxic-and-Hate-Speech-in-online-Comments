{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\n",
    "from keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder='data/3-processed_data'\n",
    "out_folder='models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(in_folder+'/X_train.csv')\n",
    "Y_train=pd.read_csv(in_folder+'/Y_train.csv')\n",
    "X_test=pd.read_csv(in_folder+'/X_test.csv')\n",
    "Y_test=pd.read_csv(in_folder+'/Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words we want to use (or: number of rows in incoming embedding vector)\n",
    "max_features = 20000 \n",
    "\n",
    "# max number of words in a comment to use (or: number of columns in incoming embedding vector)\n",
    "max_len = 200 \n",
    "\n",
    "# dimension of the embedding variable (or: number of rows in output of embedding vector)\n",
    "embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate NN model\n",
    "base_model = Sequential()\n",
    "\n",
    "# add embedding layer \n",
    "base_model.add(Embedding(input_dim=max_features, input_length=max_len,\n",
    "                         output_dim=embedding_dims))\n",
    "\n",
    "# add pooling layer \n",
    "# ... which will extract features from the embeddings of all words in the comment\n",
    "base_model.add(GlobalMaxPool1D())\n",
    "\n",
    "# add dense layer to produce an output dimension of 50 and apply relu activation\n",
    "base_model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# set the regularizing dropout layer to drop out 30% of the nodes\n",
    "base_model.add(Dropout(0.3))\n",
    "\n",
    "# finally add a dense layer\n",
    "# ... which projects output into six units and squash it with sigmoid activation\n",
    "base_model.add(Dense(7, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,566,807\n",
      "Trainable params: 2,566,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.compile(loss='binary_crossentropy',\n",
    "                   optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "\n",
    "# check the model with all our layers\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/5\n",
      "127656/127656 [==============================] - 100s 785us/step - loss: 0.0839 - acc: 0.9722 - val_loss: 0.0636 - val_acc: 0.9777\n",
      "Epoch 2/5\n",
      "127656/127656 [==============================] - 107s 840us/step - loss: 0.0580 - acc: 0.9785 - val_loss: 0.0586 - val_acc: 0.9789\n",
      "Epoch 3/5\n",
      "127656/127656 [==============================] - 108s 848us/step - loss: 0.0516 - acc: 0.9799 - val_loss: 0.0603 - val_acc: 0.9788\n",
      "Epoch 4/5\n",
      "127656/127656 [==============================] - 108s 845us/step - loss: 0.0467 - acc: 0.9815 - val_loss: 0.0648 - val_acc: 0.9766\n",
      "Epoch 5/5\n",
      "127656/127656 [==============================] - 107s 837us/step - loss: 0.0428 - acc: 0.9829 - val_loss: 0.0673 - val_acc: 0.9766\n"
     ]
    }
   ],
   "source": [
    "base_hist = base_model.fit(X_train, Y_train, batch_size=128, \n",
    "                           epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 5s 71us/step\n",
      "Test Loss:     0.11250046690034898\n",
      "Test Accuracy: 0.9534840224969718\n"
     ]
    }
   ],
   "source": [
    "base_test_loss, base_test_acc = base_model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print('Test Loss:    ', base_test_loss)\n",
    "print('Test Accuracy:', base_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "base_model.save(out_folder+'/bnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving performance\n",
    "models_performance=pd.DataFrame(columns=['Model Name', 'Test Loss', 'Test Accuracy'])\n",
    "stats=['Baseline Model', base_test_loss,base_test_acc]\n",
    "models_performance.loc[len(models_performance),:]=stats\n",
    "models_performance.to_csv(out_folder+'/models_performance.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
